{"ast":null,"code":"// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nimport { APIResource } from \"../resource.mjs\";\nexport class Completions extends APIResource {\n  create(body, options) {\n    return this._client.post('/v1/complete', {\n      body,\n      timeout: this._client._options.timeout ?? 600000,\n      ...options,\n      stream: body.stream ?? false\n    });\n  }\n}","map":{"version":3,"names":["APIResource","Completions","create","body","options","_client","post","timeout","_options","stream"],"sources":["/home/user/srt-translator/node_modules/@anthropic-ai/sdk/src/resources/completions.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nimport { APIResource } from \"../resource.js\";\nimport { APIPromise } from \"../core.js\";\nimport * as Core from \"../core.js\";\nimport * as CompletionsAPI from \"./completions.js\";\nimport * as MessagesAPI from \"./messages.js\";\nimport { Stream } from \"../streaming.js\";\n\nexport class Completions extends APIResource {\n  /**\n   * [Legacy] Create a Text Completion.\n   *\n   * The Text Completions API is a legacy API. We recommend using the\n   * [Messages API](https://docs.anthropic.com/en/api/messages) going forward.\n   *\n   * Future models and features will not be compatible with Text Completions. See our\n   * [migration guide](https://docs.anthropic.com/en/api/migrating-from-text-completions-to-messages)\n   * for guidance in migrating from Text Completions to Messages.\n   */\n  create(body: CompletionCreateParamsNonStreaming, options?: Core.RequestOptions): APIPromise<Completion>;\n  create(\n    body: CompletionCreateParamsStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<Completion>>;\n  create(\n    body: CompletionCreateParamsBase,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<Completion> | Completion>;\n  create(\n    body: CompletionCreateParams,\n    options?: Core.RequestOptions,\n  ): APIPromise<Completion> | APIPromise<Stream<Completion>> {\n    return this._client.post('/v1/complete', {\n      body,\n      timeout: (this._client as any)._options.timeout ?? 600000,\n      ...options,\n      stream: body.stream ?? false,\n    }) as APIPromise<Completion> | APIPromise<Stream<Completion>>;\n  }\n}\n\nexport interface Completion {\n  /**\n   * Unique object identifier.\n   *\n   * The format and length of IDs may change over time.\n   */\n  id: string;\n\n  /**\n   * The resulting completion up to and excluding the stop sequences.\n   */\n  completion: string;\n\n  /**\n   * The model that will complete your prompt.\\n\\nSee\n   * [models](https://docs.anthropic.com/en/docs/models-overview) for additional\n   * details and options.\n   */\n  model: MessagesAPI.Model;\n\n  /**\n   * The reason that we stopped.\n   *\n   * This may be one the following values:\n   *\n   * - `\"stop_sequence\"`: we reached a stop sequence â€” either provided by you via the\n   *   `stop_sequences` parameter, or a stop sequence built into the model\n   * - `\"max_tokens\"`: we exceeded `max_tokens_to_sample` or the model's maximum\n   */\n  stop_reason: string | null;\n\n  /**\n   * Object type.\n   *\n   * For Text Completions, this is always `\"completion\"`.\n   */\n  type: 'completion';\n}\n\nexport type CompletionCreateParams = CompletionCreateParamsNonStreaming | CompletionCreateParamsStreaming;\n\nexport interface CompletionCreateParamsBase {\n  /**\n   * The maximum number of tokens to generate before stopping.\n   *\n   * Note that our models may stop _before_ reaching this maximum. This parameter\n   * only specifies the absolute maximum number of tokens to generate.\n   */\n  max_tokens_to_sample: number;\n\n  /**\n   * The model that will complete your prompt.\\n\\nSee\n   * [models](https://docs.anthropic.com/en/docs/models-overview) for additional\n   * details and options.\n   */\n  model: MessagesAPI.Model;\n\n  /**\n   * The prompt that you want Claude to complete.\n   *\n   * For proper response generation you will need to format your prompt using\n   * alternating `\\n\\nHuman:` and `\\n\\nAssistant:` conversational turns. For example:\n   *\n   * ```\n   * \"\\n\\nHuman: {userQuestion}\\n\\nAssistant:\"\n   * ```\n   *\n   * See [prompt validation](https://docs.anthropic.com/en/api/prompt-validation) and\n   * our guide to\n   * [prompt design](https://docs.anthropic.com/en/docs/intro-to-prompting) for more\n   * details.\n   */\n  prompt: string;\n\n  /**\n   * An object describing metadata about the request.\n   */\n  metadata?: MessagesAPI.Metadata;\n\n  /**\n   * Sequences that will cause the model to stop generating.\n   *\n   * Our models stop on `\"\\n\\nHuman:\"`, and may include additional built-in stop\n   * sequences in the future. By providing the stop_sequences parameter, you may\n   * include additional strings that will cause the model to stop generating.\n   */\n  stop_sequences?: Array<string>;\n\n  /**\n   * Whether to incrementally stream the response using server-sent events.\n   *\n   * See [streaming](https://docs.anthropic.com/en/api/streaming) for details.\n   */\n  stream?: boolean;\n\n  /**\n   * Amount of randomness injected into the response.\n   *\n   * Defaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0`\n   * for analytical / multiple choice, and closer to `1.0` for creative and\n   * generative tasks.\n   *\n   * Note that even with `temperature` of `0.0`, the results will not be fully\n   * deterministic.\n   */\n  temperature?: number;\n\n  /**\n   * Only sample from the top K options for each subsequent token.\n   *\n   * Used to remove \"long tail\" low probability responses.\n   * [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).\n   *\n   * Recommended for advanced use cases only. You usually only need to use\n   * `temperature`.\n   */\n  top_k?: number;\n\n  /**\n   * Use nucleus sampling.\n   *\n   * In nucleus sampling, we compute the cumulative distribution over all the options\n   * for each subsequent token in decreasing probability order and cut it off once it\n   * reaches a particular probability specified by `top_p`. You should either alter\n   * `temperature` or `top_p`, but not both.\n   *\n   * Recommended for advanced use cases only. You usually only need to use\n   * `temperature`.\n   */\n  top_p?: number;\n}\n\nexport namespace CompletionCreateParams {\n  /**\n   * @deprecated use `Anthropic.Messages.Metadata` instead\n   */\n  export type Metadata = MessagesAPI.Metadata;\n\n  export type CompletionCreateParamsNonStreaming = CompletionsAPI.CompletionCreateParamsNonStreaming;\n  export type CompletionCreateParamsStreaming = CompletionsAPI.CompletionCreateParamsStreaming;\n}\n\nexport interface CompletionCreateParamsNonStreaming extends CompletionCreateParamsBase {\n  /**\n   * Whether to incrementally stream the response using server-sent events.\n   *\n   * See [streaming](https://docs.anthropic.com/en/api/streaming) for details.\n   */\n  stream?: false;\n}\n\nexport interface CompletionCreateParamsStreaming extends CompletionCreateParamsBase {\n  /**\n   * Whether to incrementally stream the response using server-sent events.\n   *\n   * See [streaming](https://docs.anthropic.com/en/api/streaming) for details.\n   */\n  stream: true;\n}\n\nexport declare namespace Completions {\n  export {\n    type Completion as Completion,\n    type CompletionCreateParams as CompletionCreateParams,\n    type CompletionCreateParamsNonStreaming as CompletionCreateParamsNonStreaming,\n    type CompletionCreateParamsStreaming as CompletionCreateParamsStreaming,\n  };\n}\n"],"mappings":"AAAA;SAESA,WAAW,QAAE;AAOtB,OAAM,MAAOC,WAAY,SAAQD,WAAW;EAoB1CE,MAAMA,CACJC,IAA4B,EAC5BC,OAA6B;IAE7B,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAAC,cAAc,EAAE;MACvCH,IAAI;MACJI,OAAO,EAAG,IAAI,CAACF,OAAe,CAACG,QAAQ,CAACD,OAAO,IAAI,MAAM;MACzD,GAAGH,OAAO;MACVK,MAAM,EAAEN,IAAI,CAACM,MAAM,IAAI;KACxB,CAA4D;EAC/D","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}